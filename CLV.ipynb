{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcrSXQdv+2o/nWaz8GNeFH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshit28012002/MRM-Harshit-JIo-insititute/blob/main/CLV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "uagqiCm-6ri2",
        "outputId": "d9a48257-61bc-4714-e7a1-2f38dbafda8d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "time data \"1/1/2019\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-da876f95be5c>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convert Transaction_Date to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Transaction_Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Transaction_Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%d-%m-%Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Aggregate data to calculate RFM metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0munique_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mcache_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;31m# GH#45319\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"mixed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_strptime_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     result, tz_parsed = objects_to_datetime64(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0mCall\u001b[0m \u001b[0marray_strptime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfallback\u001b[0m \u001b[0mbehavior\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m'errors'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtz_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0munit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: time data \"1/1/2019\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Online_Sales.csv'  # Update with your file path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert Transaction_Date to datetime\n",
        "df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'], format='%d-%m-%Y')\n",
        "\n",
        "# Aggregate data to calculate RFM metrics\n",
        "# Define the current date as a reference for recency calculation\n",
        "current_date = df['Transaction_Date'].max()\n",
        "\n",
        "# Group data by CustomerID\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'Transaction_Date': lambda x: (current_date - x.max()).days,  # Recency\n",
        "    'Transaction_ID': 'nunique',  # Frequency\n",
        "    'Avg_Price': lambda x: (x * df.loc[x.index, 'Quantity']).sum()  # Monetary\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns\n",
        "rfm.rename(columns={\n",
        "    'Transaction_Date': 'Recency',\n",
        "    'Transaction_ID': 'Frequency',\n",
        "    'Avg_Price': 'Monetary'\n",
        "}, inplace=True)\n",
        "\n",
        "# Add RFM scores\n",
        "rfm['R_Score'] = pd.qcut(rfm['Recency'], 4, labels=[4, 3, 2, 1])  # 4: Most recent, 1: Least recent\n",
        "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 4, labels=[1, 2, 3, 4])  # 4: Most frequent\n",
        "rfm['M_Score'] = pd.qcut(rfm['Monetary'], 4, labels=[1, 2, 3, 4])  # 4: Highest monetary value\n",
        "\n",
        "# Combine RFM scores into a single RFM_Segment\n",
        "rfm['RFM_Segment'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n",
        "rfm['RFM_Score'] = rfm[['R_Score', 'F_Score', 'M_Score']].sum(axis=1)\n",
        "\n",
        "# Label customer segments based on RFM score\n",
        "def rfm_segment(df):\n",
        "    if df['RFM_Score'] >= 9:\n",
        "        return 'Champions'\n",
        "    elif df['RFM_Score'] >= 7:\n",
        "        return 'Loyal Customers'\n",
        "    elif df['RFM_Score'] >= 5:\n",
        "        return 'Potential Loyalists'\n",
        "    else:\n",
        "        return 'Needs Attention'\n",
        "\n",
        "rfm['Segment'] = rfm.apply(rfm_segment, axis=1)\n",
        "\n",
        "# Display the RFM dataframe\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(name=\"RFM Analysis with Segments\", dataframe=rfm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/Online_Sales.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert Transaction_Date to datetime\n",
        "df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'], format='%d-%m-%Y')\n",
        "\n",
        "# Calculate the total price for each transaction\n",
        "df['Total_Price'] = (df['Quantity'] * df['Avg_Price']) + df['Delivery_Charges']\n",
        "\n",
        "# Calculate RFM metrics\n",
        "# Recency: Days since the last purchase\n",
        "recency_df = df.groupby('CustomerID').Transaction_Date.max().reset_index()\n",
        "recency_df.columns = ['CustomerID', 'LastPurchaseDate']\n",
        "recency_df['Recency'] = (pd.to_datetime('today') - recency_df['LastPurchaseDate']).dt.days\n",
        "\n",
        "# Frequency: Total number of transactions\n",
        "frequency_df = df.groupby('CustomerID').Transaction_ID.nunique().reset_index()\n",
        "frequency_df.columns = ['CustomerID', 'Frequency']\n",
        "\n",
        "# Monetary: Total spending\n",
        "monetary_df = df.groupby('CustomerID').Total_Price.sum().reset_index()\n",
        "monetary_df.columns = ['CustomerID', 'Monetary']\n",
        "\n",
        "# Merge RFM metrics into a single dataframe\n",
        "rfm_df = recency_df.merge(frequency_df, on='CustomerID').merge(monetary_df, on='CustomerID')\n",
        "\n",
        "# Rank customers on Recency, Frequency, and Monetary values\n",
        "rfm_df['R_Score'] = pd.qcut(rfm_df['Recency'], 4, ['1', '2', '3', '4'])\n",
        "rfm_df['F_Score'] = pd.qcut(rfm_df['Frequency'], 4, ['4', '3', '2', '1'])\n",
        "rfm_df['M_Score'] = pd.qcut(rfm_df['Monetary'], 4, ['4', '3', '2', '1'])\n",
        "\n",
        "# Combine R, F, M scores to get RFM score\n",
        "rfm_df['RFM_Score'] = rfm_df[['R_Score', 'F_Score', 'M_Score']].agg(''.join, axis=1)\n",
        "\n",
        "# Interpret RFM Score\n",
        "rfm_df['Customer_Segment'] = rfm_df['RFM_Score'].replace({\n",
        "    '111': 'Best Customers',\n",
        "    '112': 'Loyal Customers',\n",
        "    '113': 'Potential Loyalists',\n",
        "    '123': 'Recent Customers',\n",
        "    '124': 'Promising',\n",
        "    '134': 'Need Attention',\n",
        "    '144': 'About to Sleep',\n",
        "    '222': 'At Risk',\n",
        "    '233': 'Hibernating',\n",
        "    '244': 'Lost'\n",
        "})\n",
        "\n",
        "# Display the RFM dataframe\n",
        "print(rfm_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "3cnhRCUy7vPK",
        "outputId": "0a1d6cb4-36d9-41ee-8bb2-47e8f9677065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "time data \"1/1/2019\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-944acf26c703>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert Transaction_Date to datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Transaction_Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Transaction_Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%d-%m-%Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Calculate the total price for each transaction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0munique_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mcache_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;31m# GH#45319\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"mixed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_strptime_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     result, tz_parsed = objects_to_datetime64(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0mCall\u001b[0m \u001b[0marray_strptime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfallback\u001b[0m \u001b[0mbehavior\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m'errors'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtz_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0munit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: time data \"1/1/2019\" doesn't match format \"%d-%m-%Y\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Transaction_Date to datetime with format inference\n",
        "df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'], dayfirst=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "IqJnsb3Q8EtY",
        "outputId": "6ea187a8-2249-44b8-e178-f29886f4feed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "time data \"1/13/2019\" doesn't match format \"%d/%m/%Y\", at position 12. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-9b24c63a9cd7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert Transaction_Date to datetime with format inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Transaction_Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Transaction_Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0munique_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mcache_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m             \u001b[0;31m# GH#45319\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"mixed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_strptime_with_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     result, tz_parsed = objects_to_datetime64(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0mCall\u001b[0m \u001b[0marray_strptime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfallback\u001b[0m \u001b[0mbehavior\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m'errors'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m     \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtz_out\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0munit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mstrptime.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: time data \"1/13/2019\" doesn't match format \"%d/%m/%Y\", at position 12. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to convert with dayfirst=True and coerce errors\n",
        "df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'], format='%d/%m/%Y', errors='coerce')\n",
        "\n",
        "# Check for rows where conversion failed\n",
        "invalid_dates = df[df['Transaction_Date'].isna()]\n",
        "\n",
        "# If invalid dates exist, try converting using another format\n",
        "if not invalid_dates.empty:\n",
        "    df.loc[df['Transaction_Date'].isna(), 'Transaction_Date'] = pd.to_datetime(\n",
        "        df.loc[df['Transaction_Date'].isna(), 'Transaction_Date'],\n",
        "        format='%m/%d/%Y',\n",
        "        errors='coerce'\n",
        "    )\n",
        "\n",
        "# Check again for any remaining invalid dates\n",
        "remaining_invalid_dates = df[df['Transaction_Date'].isna()]\n",
        "if not remaining_invalid_dates.empty:\n",
        "    print(\"There are still some invalid dates. Here are the rows:\")\n",
        "    print(remaining_invalid_dates)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XexagiJY8TP3",
        "outputId": "3c549065-dce6-4437-b1b5-c1f44c5e2f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are still some invalid dates. Here are the rows:\n",
            "       CustomerID  Transaction_ID Transaction_Date     Product_SKU  \\\n",
            "1607        13468           17754              NaT  GGOEAAQB036114   \n",
            "1608        13468           17754              NaT  GGOEAAQB036115   \n",
            "1609        13468           17754              NaT  GGOEGAFB035817   \n",
            "1610        13468           17755              NaT  GGOEGBFC018799   \n",
            "1611        13468           17755              NaT  GGOEGDHB072199   \n",
            "...           ...             ...              ...             ...   \n",
            "52919       14410           48493              NaT  GGOENEBB078899   \n",
            "52920       14410           48494              NaT  GGOEGAEB091117   \n",
            "52921       14410           48495              NaT  GGOENEBQ084699   \n",
            "52922       14600           48496              NaT  GGOENEBQ079199   \n",
            "52923       14600           48497              NaT  GGOENEBQ079099   \n",
            "\n",
            "                                   Product_Description Product_Category  \\\n",
            "1607                     Android Women's Fleece Hoodie          Apparel   \n",
            "1608                     Android Women's Fleece Hoodie          Apparel   \n",
            "1609                          Google Men's  Zip Hoodie          Apparel   \n",
            "1610                       Electronics Accessory Pouch             Bags   \n",
            "1611    Google Accent Insulated Stainless Steel Bottle        Drinkware   \n",
            "...                                                ...              ...   \n",
            "52919            Nest Cam Indoor Security Camera - USA         Nest-USA   \n",
            "52920                          Google Zip Hoodie Black          Apparel   \n",
            "52921     Nest Learning Thermostat 3rd Gen-USA - White         Nest-USA   \n",
            "52922    Nest Protect Smoke + CO White Wired Alarm-USA         Nest-USA   \n",
            "52923  Nest Protect Smoke + CO White Battery Alarm-USA         Nest-USA   \n",
            "\n",
            "       Quantity  Avg_Price  Delivery_Charges Coupon_Status  \n",
            "1607          1      57.76              6.50       Clicked  \n",
            "1608          1      57.76              6.50       Clicked  \n",
            "1609          1      57.76              6.50       Clicked  \n",
            "1610          1       4.12              6.50          Used  \n",
            "1611          1      20.62              6.50       Clicked  \n",
            "...         ...        ...               ...           ...  \n",
            "52919         1     121.30              6.50       Clicked  \n",
            "52920         1      48.92              6.50          Used  \n",
            "52921         1     151.88              6.50          Used  \n",
            "52922         5      80.52              6.50       Clicked  \n",
            "52923         4      80.52             19.99       Clicked  \n",
            "\n",
            "[32315 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with invalid Transaction_Date\n",
        "df_cleaned = df.dropna(subset=['Transaction_Date'])\n",
        "\n",
        "# Continue with the RFM analysis using df_cleaned\n"
      ],
      "metadata": {
        "id": "uwvM2y7U87LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv('/content/Online_Sales.csv')\n",
        "\n",
        "# Convert Transaction_Date to datetime\n",
        "df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid Transaction_Date\n",
        "df = df.dropna(subset=['Transaction_Date'])\n",
        "\n",
        "# Calculate the total price for each transaction\n",
        "df['Total_Price'] = df['Quantity'] * df['Avg_Price']\n",
        "\n",
        "# Calculate RFM values\n",
        "# Reference date for recency calculation (e.g., the latest transaction date in the dataset)\n",
        "reference_date = df['Transaction_Date'].max() + pd.Timedelta(days=1)\n",
        "\n",
        "# Group by CustomerID and calculate RFM metrics\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'Transaction_Date': lambda x: (reference_date - x.max()).days,  # Recency\n",
        "    'Transaction_ID': 'nunique',                                   # Frequency\n",
        "    'Total_Price': 'sum'                                            # Monetary\n",
        "})\n",
        "\n",
        "# Rename columns to RFM metrics\n",
        "rfm.columns = ['Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Display the RFM table\n",
        "print(rfm.head())\n",
        "\n",
        "# Optional: Create RFM segments based on quantiles\n",
        "rfm['R'] = pd.qcut(rfm['Recency'], 4, labels=[4, 3, 2, 1])\n",
        "rfm['F'] = pd.qcut(rfm['Frequency'].rank(method='first'), 4, labels=[1, 2, 3, 4])\n",
        "rfm['M'] = pd.qcut(rfm['Monetary'], 4, labels=[1, 2, 3, 4])\n",
        "\n",
        "# Combine RFM segments into a single score\n",
        "rfm['RFM_Segment'] = rfm['R'].astype(str) + rfm['F'].astype(str) + rfm['M'].astype(str)\n",
        "rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1)\n",
        "\n",
        "# Display the RFM segments\n",
        "print(rfm.head())\n",
        "\n",
        "# Now, you can interpret the RFM segments for CLV analysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIvhv60J9Na3",
        "outputId": "8ca58779-e46e-458b-9a64-e27a905547ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Recency  Frequency  Monetary\n",
            "CustomerID                              \n",
            "12346           108          1     30.99\n",
            "12347            60         31  13834.90\n",
            "12348            74          8   1442.12\n",
            "12350            18         11   1360.07\n",
            "12356           108         13   1442.47\n",
            "            Recency  Frequency  Monetary  R  F  M RFM_Segment  RFM_Score\n",
            "CustomerID                                                              \n",
            "12346           108          1     30.99  3  1  1         311          5\n",
            "12347            60         31  13834.90  3  4  4         344         11\n",
            "12348            74          8   1442.12  3  2  2         322          7\n",
            "12350            18         11   1360.07  4  2  2         422          8\n",
            "12356           108         13   1442.47  3  3  2         332          8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define discount rate and time horizon\n",
        "r = 0.10  # Discount rate (e.g., 10%)\n",
        "N = 12    # Time horizon in months\n",
        "\n",
        "# Function to calculate CLV\n",
        "def calculate_clv(frequency, monetary, discount_rate, periods):\n",
        "    avg_revenue_per_period = monetary / frequency\n",
        "    discounted_revenue = [\n",
        "        avg_revenue_per_period / ((1 + discount_rate) ** t)\n",
        "        for t in range(1, periods + 1)\n",
        "    ]\n",
        "    return np.sum(discounted_revenue)\n",
        "\n",
        "# Add CLV calculation to the RFM DataFrame\n",
        "rfm['CLV'] = rfm.apply(\n",
        "    lambda row: calculate_clv(row['Frequency'], row['Monetary'], r, N), axis=1\n",
        ")\n",
        "\n",
        "# Display the RFM table with CLV\n",
        "print(rfm.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KutcYlcP9lxN",
        "outputId": "f70be5c6-2e5f-412f-cf56-6f5ea2650c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Recency  Frequency  Monetary  R  F  M RFM_Segment  RFM_Score  \\\n",
            "CustomerID                                                                 \n",
            "12346           108          1     30.99  3  1  1         311          5   \n",
            "12347            60         31  13834.90  3  4  4         344         11   \n",
            "12348            74          8   1442.12  3  2  2         322          7   \n",
            "12350            18         11   1360.07  4  2  2         422          8   \n",
            "12356           108         13   1442.47  3  3  2         332          8   \n",
            "\n",
            "                    CLV  \n",
            "CustomerID               \n",
            "12346        211.156310  \n",
            "12347       3040.862742  \n",
            "12348       1228.270156  \n",
            "12350        842.463440  \n",
            "12356        756.042003  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your CSV file\n",
        "df = pd.read_csv('/content/Online_Sales.csv')\n",
        "\n",
        "# Convert Transaction_Date to datetime\n",
        "df['Transaction_Date'] = pd.to_datetime(df['Transaction_Date'], errors='coerce')\n",
        "\n",
        "# Drop rows with invalid Transaction_Date\n",
        "df = df.dropna(subset=['Transaction_Date'])\n",
        "\n",
        "# Calculate the total price for each transaction\n",
        "df['Total_Price'] = df['Quantity'] * df['Avg_Price']\n",
        "\n",
        "# Calculate RFM values\n",
        "reference_date = df['Transaction_Date'].max() + pd.Timedelta(days=1)\n",
        "rfm = df.groupby('CustomerID').agg({\n",
        "    'Transaction_Date': lambda x: (reference_date - x.max()).days,  # Recency\n",
        "    'Transaction_ID': 'nunique',                                   # Frequency\n",
        "    'Total_Price': 'sum'                                            # Monetary\n",
        "})\n",
        "rfm.columns = ['Recency', 'Frequency', 'Monetary']\n",
        "\n",
        "# Calculate CLV using the formula provided\n",
        "r = 0.10  # Discount rate (e.g., 10%)\n",
        "N = 12    # Time horizon in months\n",
        "\n",
        "def calculate_clv(frequency, monetary, discount_rate, periods):\n",
        "    avg_revenue_per_period = monetary / frequency\n",
        "    discounted_revenue = [\n",
        "        avg_revenue_per_period / ((1 + discount_rate) ** t)\n",
        "        for t in range(1, periods + 1)\n",
        "    ]\n",
        "    return np.sum(discounted_revenue)\n",
        "\n",
        "rfm['CLV'] = rfm.apply(\n",
        "    lambda row: calculate_clv(row['Frequency'], row['Monetary'], r, N), axis=1\n",
        ")\n",
        "\n",
        "# Optional: Create RFM segments based on quantiles\n",
        "rfm['R'] = pd.qcut(rfm['Recency'], 4, labels=[4, 3, 2, 1])\n",
        "rfm['F'] = pd.qcut(rfm['Frequency'].rank(method='first'), 4, labels=[1, 2, 3, 4])\n",
        "rfm['M'] = pd.qcut(rfm['Monetary'], 4, labels=[1, 2, 3, 4])\n",
        "\n",
        "rfm['RFM_Segment'] = rfm['R'].astype(str) + rfm['F'].astype(str) + rfm['M'].astype(str)\n",
        "rfm['RFM_Score'] = rfm[['R', 'F', 'M']].sum(axis=1)\n",
        "\n",
        "# Display the RFM table with CLV\n",
        "print(rfm.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pX1TXGS-Ac6",
        "outputId": "48017f4d-a2a5-42de-de2b-3587ea4268a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Recency  Frequency  Monetary          CLV  R  F  M RFM_Segment  \\\n",
            "CustomerID                                                                   \n",
            "12346           108          1     30.99   211.156310  3  1  1         311   \n",
            "12347            60         31  13834.90  3040.862742  3  4  4         344   \n",
            "12348            74          8   1442.12  1228.270156  3  2  2         322   \n",
            "12350            18         11   1360.07   842.463440  4  2  2         422   \n",
            "12356           108         13   1442.47   756.042003  3  3  2         332   \n",
            "\n",
            "            RFM_Score  \n",
            "CustomerID             \n",
            "12346               5  \n",
            "12347              11  \n",
            "12348               7  \n",
            "12350               8  \n",
            "12356               8  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/bank marketing - logistic (1) (1).xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Identify categorical variables\n",
        "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
        "\n",
        "# Convert categorical variables to dummy variables\n",
        "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Separate the independent variables (X) and the dependent variable (y)\n",
        "X = df.drop(columns=['y'])\n",
        "y = df['y'].apply(lambda x: 1 if x == 'yes' else 0)  # Convert 'yes'/'no' to 1/0\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation results\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwpAIP9lAI0u",
        "outputId": "c2bb4f26-d1f7-4d2a-82be-c831a229843d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.901657458563536\n",
            "Confusion Matrix:\n",
            "[[789  18]\n",
            " [ 71  27]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.98      0.95       807\n",
            "           1       0.60      0.28      0.38        98\n",
            "\n",
            "    accuracy                           0.90       905\n",
            "   macro avg       0.76      0.63      0.66       905\n",
            "weighted avg       0.88      0.90      0.88       905\n",
            "\n"
          ]
        }
      ]
    }
  ]
}